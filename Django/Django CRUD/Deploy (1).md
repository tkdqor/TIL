## Deploy (1)
- 지금까지는 컴퓨터 개인 환경, 로컬 환경에서 django를 사용해 웹 서비스를 개발하는 방법에 대해서 배웠다.
- 서비스를 배포하기 위해서는 365일 항상 전기를 공급받을 수 있고 항상 안정적인 네트워크에 접속할 수 있는 환경이 필요.
  - 이를 위해서 과거에는 IDC라고 하는 Internet Data Center에 우리의 server 컴퓨터를 입주시켜야 했다. 우리가 개발했던 django와 같이 server용 프로그램이 돌아가는 컴퓨터는 모니터도 필요없고 연산만 빠르게
    처리하면 된다. 
  - 문제는 개인이나 소규모 스타트업의 경우 서비스 규모도 자주 바뀌게 되고 그 때마다 컴퓨터 하드웨어를 매번 구입할 수도 없다. 컴퓨터 하드웨어 전문가가 아닌 이상 이런 server를 직접 구축하고 IDC에 입주하는 것 자체가 
    굉장히 어렵고 오래걸리는 작업이다.
    
- 하지만, 이러한 문제는 아마존의 AWS를 비롯해서 Microsoft의 Azure 등 다양한 클라우드 서비스가 나타나면서 자연스럽게 해결되었다. 그래서 우리는 이제 가상의 컴퓨터를 발급받아서 실제 IDC에 입주한 것처럼 사용할 수 있게 되었다.
  - 아마존의 EC2는 Amazon Elastic Compute Cloud의 약자로 AWS에서 제공하는 가장 기본적인 컴퓨팅 유닛이다. 아마존으로부터 실제 컴퓨터처럼 사용가능한 가상 컴퓨터를 발급받을 수 있다. 
  - 우리는 EC2를 사용해 365일 항상 django가 동작할 수 있는 server로 사용할 수 있다.

- 이러한 EC2 server내에 django를 우리에게 익숙한 development 모드 server의 형태로 구성하면 -> server 내에 django가 설치되어 있고 DBMS도 디폴트인 SQLite를 그대로 사용하게 된다. 이 형태로 
  사용자에게 웹서비스를 제공할 수도 있지만 server가 교체되거나 우리의 프로젝트 소스 코드를 재배포하는 과정에서 파일 DB인 SQLite 파일이 삭제될 경우, 서비스 데이터가 보존되지 않는다는 단점이 있다.
  - 그래서, django가 돌아가는 server 외부에 PostgreSQL과 같은 production에 적합한 고성능 RDBMS를 분리해서 사용하는 게 바람직한 구조이다.
  - 하지만, 여전히 django development 모드의 server는 다수의 트래픽을 처리하는 데 있어 안정성이나 퍼포먼스 측면에서 적합하지 않다.


### Production 환경
- 그래서 Production 환경에서는, 일반적으로 gunicorn이라는 별도의 프로그램을 사용한다.
  - 해당 프로그램은 server 리소스를 활용하여 퍼포먼스를 증대시킴과 동시에 django 프로세스를 모니터링하고 고가용성을 도모할 수 있게 한다.

- 그리고 web server인 nginx를 추가로 사용해서 static 파일과 관련된 처리는 nginx가, 그 외의 나머지 트래픽은 gunicorn를 통해 django가 대신 처리할 수 있게끔 전달해줘서 조금 더 안정적으로 트래픽을
  트래픽을 처리할 수 있게끔 만들어줄 수 있다.
  - ex) **(EC2 server -> nginx -> gunicorn -> django) -> RDBMS (Production 환경에 적합한 구조)**
    - **앞의 괄호까지는 django server / 뒤에는 RDBMS 전용 server**

- 이 상태에서 우리의 웹서비스가 더욱 성장해서 훨씬 더 많은 트래픽이 발생한다면 어떻게 될까? 우리의 서비스가 커질수록 EC2 server 한 대로는 감당하기 어려워진다.


### ELB(Elastic Load Balancer)
- 그래서 필요한 것이 바로, ELB(Elastic Load Balancer)이다. 
- 우리가 웹 서비스를 이용하는 과정에서 발생하는 데이터 통신들, 그러한 통신의 교통량을 우리는 트래픽이라고 부르는데 server는 그러한 트래픽을 열심히 처리해야하고 server 입장에서 이 트래픽은 처리해야 할 짐 덩어리이다.
  - 트래픽이 많으면 많을수록 server 입장에서는 부하, 즉 load가 걸리게 된다. 아무리 비싸고 좋은 server 한 대를 사용한다 할지라도 언젠가는 모든 트래픽을 혼자서 처리하기 어려운 시점이 온다.
  - 이러한 시점이 우리 웹서비스가 한단계 더 성장하는 계기가 되는데, server는 터지고 서비스는 장애가 발생하고 혼란스럽겠지만 그만큼 우리 서비스를 이용하는 사용자가 늘었다는 증거이기도 하다.

- 그래서 이럴 때는 트래픽을 server 한 대가 전부 다 받아서 처리하는 것이 아니라 -> 여러 대의 server를 두고 트래픽을 여러 대의 server로 골고루 분산시켜서 트래픽을 좀 더 효율적이고 안정적으로 처리할 필요가 있다.
  - 이러한 용도로 사용되는 장치, 문자 그대로 load를 분산시켜서 균형, balance를 맞춰주는 장치를 우리는 load balancer라고 부른다.

- 이 load balancer는 트래픽을 한 곳에서 받아서 여러 server에 분산시킬 목적으로 사용하는 장치이고 AWS에서는 load balancer로 ELB, Elastic Load Balancer라는 것을 제공한다.
  - **ex) client -> ELB -> EC2 여러 대**
  - 이렇게 EC2 여러 대를 사용해서 server 클러스터를 구축한 다음, 모든 서비스 트래픽을 ELB가 앞단에서 먼저 받고 클러스터 내의 server들에게 트래픽을 골고루 분산시켜준다. 즉, 클러스터 내에 server가 n대 있다면 각 EC2 인스턴스는 트래픽을 약 n분의 1만 받게 된다. 
  - 이러한 구조에서는 우리의 서비스가 성장하면서 트래픽이 늘어남에 따라 단순히 server를 추가해주는 것만으로도 계속해서 더 많은 트래픽을 감당할 수 있게 된다. 우리가 사용하고 있는 대부분의 웹 서비스들이 다음과 같은 형태로 10만 유저, 100만 유저 이상을 안정적으로 처리하고 있다.


### HTTPS
- 지금까지 우리가 배운 방식은 전부 HTTP 기반이었다. 하지만, 요즘은 모든 웹서비스가 보안을 위해서 HTTPS를 사용한다. HTTPS는 HTTP over Secure Socket Layer의 약자로 그냥 일반 HTTP보다 보안이 강화된 것이라고 생각하면 된다.
- 우리가 지금까지 사용했던 일반 HTTP는 보안에 굉장히 취약하다. 그 이유는, HTTP에서는 모든 통신이 암호화되어 있지 않은 상태로 전송이 되기 때문에 아주 쉽게 원문을 들여다 볼 수 있기 떄문이다.
  - HTTPS는 HTTP 통신이 암호화된 상태로 전송이 되기 때문에 내용을 들여다봐도 무슨말인지 모르게 되어서 더욱 더 안전하다. HTTPS를 적용하지 않은 웹 페이지에 사용자가 접근할 경우, 브라우저 레벨에서 "연결이 비공개로 설정되어 있지 않습니다."라는 경고를 띄우게 된다. 우리가 힘들게 만든 웹 서비스를 배포해서 자랑하기 위해 링크를 보냈는데, 링크를 누르자마자 다음과 같은 경고를 띄운다면 안 된다.

- 그래서 HTTPS는 선택이 아니라, 반드시 적용해야 한다.



